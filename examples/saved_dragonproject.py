# -*- coding: utf-8 -*-
"""saved_dragonProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ykVWoWX2mFxzRPeVrmYzSlHudNQR-nH
"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2


# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/My Drive/causal_project


# Commented out IPython magic to ensure Python compatibility.
# %cd causalml



# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.linear_model import LogisticRegressionCV, LogisticRegression
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error as mse
from scipy.stats import entropy
import warnings

from causalml.inference.meta import LRSRegressor
from causalml.inference.meta import XGBTRegressor, MLPTRegressor
from causalml.inference.meta import BaseXRegressor, BaseRRegressor, BaseSRegressor, BaseTRegressor
from causalml.inference.tf import DragonNet
from causalml.match import NearestNeighborMatch, MatchOptimizer, create_table_one
from causalml.propensity import ElasticNetPropensityModel
from causalml.dataset.regression import *
from causalml.metrics import *

import os, sys

# %matplotlib inline

warnings.filterwarnings('ignore')
plt.style.use('fivethirtyeight')
sns.set_palette('Paired')
plt.rcParams['figure.figsize'] = (12,8)

# random sampling of the dataset for confidence intervals
# random.seed(42)
# df = pd.read_csv(f'data/ars_jvm_cut.csv', 
#            skiprows=lambda x: x > 0 and random.random() >=0.05)

df = pd.read_csv(f'data/ars_jvm_cut.csv', header=1)
cols =  ["match", "term", "range","fuzzy", "prefix", "wildcard", "bool", "match_all", "exists",  "request_length", "num_keys","outstandingRequests","responseTime", "serviceTime", "treated", "cpu", "memory", "jvm_memory"] 
df.columns = cols
cols


df.shape

df.head()

df = df.drop('memory', axis=1)
df.head()

pd.Series(df['treated']).value_counts(normalize=True)



X = df.loc[:,'match': 'serviceTime']
X

treatment = df['treated']
p_model = ElasticNetPropensityModel()
p = p_model.fit_predict(X, treatment)

treatment = df['treated']
y = df['jvm_memory']

s_learner = BaseSRegressor(LGBMRegressor())
s_ate = s_learner.estimate_ate(X, treatment, y)[0]
s_ite = s_learner.fit_predict(X, treatment, y)

t_learner = BaseTRegressor(LGBMRegressor())
t_ate = t_learner.estimate_ate(X, treatment, y)[0][0]
t_ite = t_learner.fit_predict(X, treatment, y)

# x_learner = BaseXRegressor(LGBMRegressor())
# x_ate = x_learner.estimate_ate(X, treatment, y, p)[0][0]
# x_ite = x_learner.fit_predict(X, treatment, y, p)

# r_learner = BaseRRegressor(LGBMRegressor())
# r_ate = r_learner.estimate_ate(X, treatment, y, p)[0][0]
# r_ite = r_learner.fit_predict(X, treatment, y, p)

dragon = DragonNet(neurons_per_layer=200, targeted_reg=True)

dragon_ite = dragon.fit_predict(X, treatment, y, return_components=False)
dragon_ate = dragon_ite.mean()

df_preds = pd.DataFrame([s_ite.ravel(),
                          t_ite.ravel(),
                          dragon_ite.ravel(),
                          treatment.ravel(),
                          y.ravel()],
                       index=['S','T','dragonnet','w','y']).T

df_cumgain = get_cumgain(df_preds)

df_result = pd.DataFrame([s_ate, t_ate,  dragon_ate, y.ravel()],
                     index=['S','T','dragonnet','actual'], columns=['ATE'])
#df_result['MAE'] = [mean_absolute_error(t,p) for t,p in zip([s_ite, t_ite, x_ite, r_ite, dragon_ite])
                                                            

df_result['AUUC'] = auuc_score(df_preds)

df_result

plot_gain(df_preds)

import tensorflow as tf
model_save_name = 'dragon.pt'
path = F"{model_save_name}" 

dragon_ite.tofile(path)
